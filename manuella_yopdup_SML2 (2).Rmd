---
title: "Statistical Machine Learning for Data Science Assignment 2"
author: "Manuella Kristeva NAKAM YOPDUP"
date: "2024-12-13"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
suppressMessages(suppressPackageStartupMessages(suppressWarnings({
library(class)
library(rpart)
library(rpart.plot)
library(ROCR)
library(ggplot2)
library(dplyr)
library(corrplot)
library(reshape2)
library(dslabs) # Package by Yann LeCun to provide the MNIST data
library(class)
library(MASS)
})))

```


## Exercise 1: Practical SML on DNA Microarrays

```{r, tidy=TRUE, results='markup'}
prostate <- read.csv("~/Desktop/sml/prostate-cancer-1.csv", header=TRUE)
#head(prostate)
```

## 1. Comment on the shape of this dataset in terms of the sample size and the dimensionality of the input space

```{r}
n = dim(prostate)[1]
p = dim(prostate)[2] - 1
cat("The size  n  of the dataset is: ", n)
cat("\nThe dimensionality  p  of the dataset is: ", p)

```

n is too small compare to p so, we can face to the problem of curse of dimensionnality(COD) refers to the problem that the space of possible sets of parameter values grows exponentially with the number of unknown parameters, severely impairing the search for the globally optimal parameter values.

The curse of dimensionnality can implies that the distance become  meaningless if we are using kernel model.
Also, COD can cause overfitting



## 2. Comment succinctly from the statistical perspective on the type of data in the input space

```{r, echo=FALSE, results='hide'}
summary(prostate)
```
By visualization of the summary of the variables, we observe that:

All the variables have almost the same scale that implies we don't need to normalize the data . Also, because we are in high-dimensional space, this uniform scaling can help to reduce overfitting and avoid that a single feature influences the model much more than the others.



```{r, echo=FALSE, results= 'hide'}
# Check the duplicated
duplicates <- prostate[duplicated(prostate) | duplicated(prostate, fromLast = TRUE), ]
print(duplicates)

```


## 3. Plot the distribution of the response for this dataset and comment.

```{r}
prostate$Y <- as.factor(prostate$Y)

ggplot(prostate, aes(x = Y, col = y)) + 
  geom_bar(color = "lightblue", fill = "lightblue") +
  labs(title = "Distribution of Y", x= "Prostate Cancer", y="Frequency")+
  theme_minimal()
  
  

```

The classes are almost proportional in the dataset, so it's not an imbalance dataset. This can avoid that the model learn more the distribution of the majority class and then reduces the risk of bias towards the majority class.The advantages

This will be good for our models because train model on balanced datasets tend to generalize better on unknown dataset. 



## 4. Identify the 9 individually most powerful predictor variables with respect to the response according the Kruskal-Wallis test statistic

The Kruskal-Wallis H test (sometimes also called the "one-way ANOVA on ranks") is a rank-based nonparametric test that can be used to determine if there are statistically significant differences between two or more groups of an independent variable on a continuous or ordinal dependent variable.

```{r}
kruskal_results <- sapply(prostate[, -which(names(prostate) == "Y")], function(x){
  kruskal.test(x ~ prostate$Y)$statistic
})

kruskal_df <- data.frame(Variable = names(kruskal_results), Statistic = kruskal_results)

kruskal_df$Variable <- sub("\\..*", "", kruskal_df$Variable)

kruskal_df <- kruskal_df %>% filter(is.finite(Statistic))

power_predictors <- kruskal_df %>% arrange(desc(Statistic)) %>% top_n(9, Statistic)
print(power_predictors)


```




## 5. Generate a type=’h’ plot with the Kruskal-Wallis test statistic as the y-axis and the variable name as the x-axis

```{r}
plot(1:nrow(power_predictors), power_predictors$Statistic, type = 'h',
     main="Kruskal-Wallis test statistic for powerful predictors",
     xlab="Kruskal-Wallis variables",
     ylab="Kruskal-Wallis test statistic",
     xaxt='n',
     col = "red",
     ylim= c(0, max(power_predictors$Statistic)*1.1))
axis(1, at=1:nrow(power_predictors), labels = power_predictors$Variable, las=2, cex=0.2)


```


## 6. Generate the comparative boxplots of the 9 most powerful variable with respect to the response and comment on what you observe.

```{r}

subvar <- power_predictors$Variable

 opar <- par(mfrow=c(2,3))
 
 for(j in subvar)
 {
    boxplot(prostate[,j]~prostate[,1], col=as.numeric(prostate[,1])+2,
       xlab = "Prostate",
       ylab = j)
 }  
 
 par(opar)
```

We can observe that most of the features selected are statistically differents depending on the two classes. For example, features like X217844_at, X212640_at and X211935_at show distinct separation between classes 0 and 1,  suggesting that they may be informative for classification tasks. 



## 7. Build the classification tree with cp=0.01

```{r}
tree.xy <- rpart(Y~., data=prostate, cp= 0.01)
```

```{r}
Ytrue<- prostate$Y
Yhat.tree <- predict(tree.xy, prostate[, -1], type='class')
table(Ytrue, Yhat.tree)
```


## • Plot the tree you just built

```{r}
prp(tree.xy)
```


## • Determine the number of terminal nodes

There is 4 terminal nodes in the tree that means that the complexity of the tree for cp=0.01 is 4

## • Write down in mathematical form region 2 and Region 4.

The second region is:

\[
  \mathbf{R_2} = \{\mathbf{X} \in \mathbb{R}^{500} / X201290 \geq 1.1 \quad \text{and} \quad X214008 \geq -0.29 \}
\]

The fourth region is:

\[
  \mathbf{R_4} = \{\mathbf{X} \in \mathbb{R}^{500} / X201290 < 1.1 \quad \text{and} \quad X209048 < -0.063 \}
\]

## • Comment on the variable at the root of the tree in light of the Kruskal-Wallis statistic

We observe that the Root Variable is not the first most significant variable but the fourth one and the first one doesn't appear in the tree. This is because :

The variable at the root of the decision tree represents the first split that maximally separates the data into distinct groups based on the target variable. This variable is chosen to minimize the impurity .

In contrast, the most significant variable in the context of the Kruskal-Wallis statistic is determined by comparing the medians of the groups. It reflects the variable that contributes most to differences across the groups based on rank sums, rather than just the first split.

Conclusion, the most significant variable in the context of the Kruskal-Wallis statistic are not always the variable that minimize the impurity.


## 8. Generate the comparative boxplots of the 9 weakest variable with respect to the response and comment on what you observe.

```{r}
weak_predictors <- kruskal_df %>% arrange(desc(Statistic)) %>% top_n(-9, Statistic)
print(weak_predictors)
```

```{r}
subvar <- weak_predictors$Variable

 opar <- par(mfrow=c(2,3))
 
 for(j in subvar)
 {
    boxplot(prostate[,j]~prostate[,1], col=as.numeric(prostate[,1])+2,
       xlab = "Prostate",
       ylab = j)
 }  
 
 par(opar)
```

We observe that these don't allow us to separate clearly the two classes because there are not statistically significantly differents between classes for all of those variables.

# 9. Generate the correlation plot of the predictor variables and comment extensively on what they reveal, if anything.

## Correlation matrix of the most powerful variables

```{r}

cor_data <- prostate[, power_predictors$Variable]
corr.x <- cor(cor_data, use = "complete.obs")
corrplot(corr.x)
```
We observe that almost all the variables are correlated with the others, so they can provide redundant information.  Then among all the variables selected by Kruskall are redundant.


## Correlation matrix of the most weaker variables

```{r}
cor_data <- prostate[, weak_predictors$Variable]
corr.x <- cor(cor_data, use = "complete.obs")
corrplot(corr.x)
```

We observe that all the variables selected by Kruskall as weak are undirectly correlated with the others, so they can provide redundant information.  Then among all the variables selected by Kruskall as weakest are redundant.

## 10. Compute the eigen decomposition of the correlation matrix and comment on the ratio $\lambda_{max}/ \lambda_{min}$.


```{r}
#Corelation matrix

cor_data <- prostate[, power_predictors$Variable]
corr.x <- cor(cor_data, use = "complete.obs")

#corr.x <- cor(prostate[, -which(names(prostate) == "Y")], use = "complete.obs")


eigen_decomp <- eigen(corr.x)
eigen_decomp


lambda_max <- max(eigen_decomp$values)
print(paste("lambda_max:", lambda_max))
lambda_min <- min(eigen_decomp$values)
print(paste("lambda_min:", lambda_min))

# Comput the ratio
ratio <- lambda_max / lambda_min

# Afficher le ratio
print(paste("Ratio ;lambda_max/lambda_min:", ratio))

```
We have a  high maximum eigenvalue combined with a low minimum eigenvalue and a substantial ratio suggests that there are dominant features that explain the majority of the variance, while some dimensions may contribute little. 

Particularly, we have a big ratio that implies  that the first component is more important the last one. 

#11. Using the whole data for training and the whole data for test, build the above six learning
#machines, then plot the comparative ROC curves on the same grid


```{r, tidy=TRUE, results='markup'}
# Function to compute test errors
compute_errors <- function(train_data, test_data, k) {
  # k-NN model
  predicted <- knn(train = train_data[, -which(names(train_data) == "Y")],
                   test = test_data[, -which(names(test_data) == "Y")],
                   cl = train_data$Y,
                   k = k)
  return(mean(predicted != test_data$Y)) # Calculate the error rate
}

# Function to create and evaluate decision trees
compute_tree_error <- function(train_data, test_data, cp) {
  tree_model <- rpart(Y ~ ., data = train_data, control = rpart.control(cp = cp))
  predicted <- predict(tree_model, test_data, type = "class")
  return(mean(predicted != test_data$Y)) # Calculate the error rate
}
```

```{r}
set.seed(19671210)
#train_index <- sample(1:nrow(prostate), 0.7 * nrow(prostate)) # 70% for training
train_data <- prostate
test_data <- prostate

# Test errors for k-NN
errors_1NN <- replicate(100, compute_errors(train_data, test_data, 1))
errors_7NN <- replicate(100, compute_errors(train_data, test_data, 7))
errors_9NN <- replicate(100, compute_errors(train_data, test_data, 9))

# Test errors for decision trees
errors_tree_cp0 <- replicate(100, compute_tree_error(train_data, test_data, 0))
errors_tree_cp0.05 <- replicate(100, compute_tree_error(train_data, test_data, 0.05))
errors_tree_cp0.1 <- replicate(100, compute_tree_error(train_data, test_data, 0.1))
```


```{r}
# Function to plot ROC curves with colors
plot_roc_curves <- function(test_data, model_type, k = NULL, cp = NULL, color) {
  if (model_type == "knn") {
    # Predictions for k-NN
    predicted <- knn(train = train_data[, -which(names(train_data) == "Y")],
                     test = test_data[, -which(names(test_data) == "Y")],
                     cl = train_data$Y,
                     k = k, prob = TRUE)
    
    # Calculate predicted probabilities
    predicted_prob <- attr(predicted, "prob")
    predicted_prob <- ifelse(predicted == "1", predicted_prob, 1 - predicted_prob)

  } else if (model_type == "tree") {
    # Decision tree predictions
    tree_model <- rpart(Y ~ ., data = train_data, control = rpart.control(cp = cp))
    predicted_prob <- predict(tree_model, test_data, type = "prob")[, 2]
  }

  # Create ROC curve
  pred <- prediction(predicted_prob, test_data$Y)
  perf <- performance(pred, "tpr", "fpr")

  # Plot ROC curve with specified color
  plot(perf, col = color, add = TRUE)
}

# Initial plot setup
par(fig=c(0, 0.8, 0, 1), mar=c(4, 4, 2, 1)) # Set margins
plot.new()
plot.window(xlim=c(0, 1), ylim=c(0, 1))
abline(a=0, b=1, col="red", lty=2) # Diagonal line

# Plot ROC curves for all models with different colors
plot_roc_curves(prostate, model_type = "knn", k = 1, color = "red") # For k-NN
plot_roc_curves(prostate, model_type = "knn", k = 7, color = "blue") # For k-NN
plot_roc_curves(prostate, model_type = "knn", k = 9, color = "orange") # For k-NN
plot_roc_curves(prostate, model_type = "tree", cp = 0, color = "green")
plot_roc_curves(prostate, model_type = "tree", cp = 0.05, color = "brown")
plot_roc_curves(prostate, model_type = "tree", cp = 0.1, color = "purple")

# Add axes and labels
axis(1, at=seq(0, 1, by=0.1)) # X-axis
axis(2, at=seq(0, 1, by=0.05)) # Y-axis

# Add title and labels
title(main="ROC Curves for Different Models", col.main="black", font.main=4)
mtext("False Positive Rate", side=1, line=3)
mtext("True Positive Rate", side=2, line=3)

legend("bottomright", legend=c("1-NN","7-NN", "9NN", "Tree (cp=0)", "Tree (cp=0.05)", "Tree (cp=0.1)"),
       col=c("red", "blue", "orange", "green", "brown", "purple"), lty=1)
```



## 12. Plot all the three classification tree grown, using the prp function for the package rpart.plot

```{r}
par(mfrow = c(2, 3)) 

# Tree with cp = 0
tree_cp0 <- rpart(Y ~ ., data = train_data, control = rpart.control(cp = 0))
prp(tree_cp0, main = "Tree (cp = 0)", extra = 104, fallen.leaves = TRUE)

# Tree with cp = 0.05
tree_cp005 <- rpart(Y ~ ., data = train_data, control = rpart.control(cp = 0.05))
prp(tree_cp005, main = "Tree (cp = 0.05)", extra = 104, fallen.leaves = TRUE)

# Tree with cp = 0.1
tree_cp01 <- rpart(Y ~ ., data = train_data, control = rpart.control(cp = 0.1))
prp(tree_cp01, main = "Tree (cp = 0.1)", extra = 104, fallen.leaves = TRUE)

# Reset plotting layout
par(mfrow = c(1, 1)) # Reset to default single plot
```


## 13. Comment succinctly on what the ROC curves reveal for this data and argue in light of theory whether or not that was to be expected.

- 1NN : is the model with the large Area Under the Curve, so the best one; that is normal because 1NN is the most complex kNN function and the risk on training dataset is null

- The roc curve of the tree with cp = 0.05 and cp = 0.1 are the same that implies that we have the same complexity

## 14. Using set.seed(19671210) along with a 7/10 training 3/10 test basic stochastic holdout split of the data, compute S = 100 replicated random splits of the test error for all the above learning machines.

```{r}

 stratified.holdout <- function(y, ptr)
   {
     n              <- length(y)
     labels         <- unique(y)       # Obtain classifiers
     id.tr          <- id.te <- NULL
     # Loop once for each unique label value
  
     y <- sample(sample(sample(y)))
  
     for(j in 1:length(labels)) 
     {
      sj    <- which(y==labels[j])  # Grab all rows of label type j  
      nj    <- length(sj)           # Count of label j rows to calc proportion below
    
      id.tr <- c(id.tr, (sample(sample(sample(sj))))[1:round(nj*ptr)])
  }                               # Concatenates each label type together 1 by 1
  
  id.te  <- (1:n) [-id.tr]          # Obtain and Shuffle test indices to randomize                                
  
  return(list(idx1=id.tr,idx2=id.te)) 
}  
```





```{r}
set.seed(19671210) # Set seed for reproducibility

# Initialize storage for test errors
errors_1NN <- numeric(100)
errors_7NN <- numeric(100)
errors_9NN <- numeric(100)
errors_tree_cp0 <- numeric(100)
errors_tree_cp005 <- numeric(100)
errors_tree_cp01 <- numeric(100)

# Perform 100 replicated random splits
for (i in 1:100) {
  # Create a random split
  train_index <- sample(1:nrow(prostate), 0.7 * nrow(prostate)) # 70% for training
  train_data <- prostate[train_index, ]
  test_data <- prostate[-train_index, ]
  
  # Compute test errors for k-NN models
  errors_1NN[i] <- compute_errors(train_data, test_data, k = 1)
  errors_7NN[i] <- compute_errors(train_data, test_data, k = 7)
  errors_9NN[i] <- compute_errors(train_data, test_data, k = 9)
  
  # Compute test errors for decision trees
  errors_tree_cp0[i] <- compute_tree_error(train_data, test_data, cp = 0)
  errors_tree_cp005[i] <- compute_tree_error(train_data, test_data, cp = 0.05)
  errors_tree_cp01[i] <- compute_tree_error(train_data, test_data, cp = 0.1)
}

# Combine errors into a data frame for plotting
errors <- data.frame(
  Model = rep(c("1-NN", "7-NN", "9-NN", "Tree (cp=0)", "Tree (cp=0.05)", "Tree (cp=0.1)"),
              each = 100),
  Error = c(errors_1NN, errors_7NN, errors_9NN, errors_tree_cp0, 
            errors_tree_cp005, errors_tree_cp01)
)
#errors


```


## • Plot the comparative boxplots (be sure to properly label the plots)

```{r}
# Plotting boxplots of test errors


ggplot(errors, aes(x = Model, y = Error, fill = Model)) +
  geom_boxplot() +
  labs(title = "Test Error Distribution by Model", x = "Model", y = "Test Error Rate") +
  theme_minimal()
```


## • Comment on the distribution of the test error in light of (implicit) model complexity.

The test error is statistically the same for all the degree of complexity for KNN and the same for all the degree of the tree.

For 1-NN, 7-NN, 9-NN: The test error is statistically the same for all the degree of complexity

For Decision Trees(Tree (cp=0.05), Tree (cp=0.1), Tree (cp=0)): The test error is statistically the same for all the degree of complexity and the variability in their boxplots can indicate sensitivity to these parameters.



## • Perform a basic analysis of variance (ANOVA) on those test errors and comment!

```{r}

aov.method <- aov(Error~Model, data=errors)
anova(aov.method)
TukeyHSD(aov.method, ordered = TRUE)
```

The ANOVA table indicates that there  is statistically significance (p-value = 4.02e-06) between the models. This means that the model as a whole is a good fit for the data.

There are significant differences between trees pruned at different complexity parameters (cp). For example, comparing trees pruned at cp=0.1 and cp=0, the difference is statistically significant.

The difference between Tree (cp=0.05)-Tree (cp=0.1), Tree (cp=0)-Tree (cp=0.1) , Tree (cp=0)-Tree (cp=0.05), 1-NN-9-NN, 9-NN-7-NN  and 1-NN-7-NN is not statistically significant.

The difference between 1-NN-7-NN, Tree (cp=0.05)-7-NN, Tree (cp=0)-7-NN and Tree (cp=0.1)-7-NN is statistically significant.

Overall, the analysis suggests that the choice of tree pruning level significantly impacts the model's performance. 

## 15. Comment extensively on the most general observation and lesson you gleaned from this exploration.

In this analysis, we have observed that: despite the high dimensionality of our dataset, a remarkably small subset of three variables (X201290_at, X209048_s_at, and X214008_at) proved sufficient for accurate classification. This finding underscores the critical role of feature selection in  complex models and enhancing their interpretability.

The decision tree, in particular, excels at revealing the importance of different features. By examining the tree structure, we can identify the variables that contribute most to the classification process. This knowledge can be invaluable for domain experts, as it can provide insights into the underlying mechanisms driving the phenomenon of interest.

Also, the tree-based models (pruned at different complexity parameters) appear to have significantly different performance compared to the k nearest neighbors. This suggests that, for this particular dataset, tree-based models might be more suitable for classification tasks.

But the models have been train and test on the same dataset, so it's not enough to conclude on which model will be the best on test dataset.


## Exercise 2: Nearest Neighbors Method for Digit Recognition

```{r}

mnist <- read_mnist() # Read in the MNIST data
```


# Part 1: Multi-class classification on MNIST

Throughout this part of the exercise, you will perform multiclass classification in the MNIST
data using the learning machines 1NN, 5NN, 7NN, 9NN, and 13NN.

## 1. Write down in mathematical form the expression of $\widehat{f}_{kNN}(x)$, the prediction function of the kNN learning machine.

\[
  \widehat{f}_{kNN}(x) = \arg\max_{g \in \{1, \ldots, G\}} \,{p_{(k)}^{g}(x)} 
\]

Where 

\[
  p_{(k)}^{g}(x) = \frac{1}{k} \sum_{i=1}^{n} \mathbf{1}(x_i \in V_k(x)) \mathbf{1}(Y_i = g)
\]


## 2. Choose n a training set size and m a test set size, and write a piece of code for sampling a fragment from the large dataset. Explain why you choose the numbers you choose.

```{r}
stratified.holdout <- function(y, ptr)
   {
     n              <- length(y)
     labels         <- unique(y)       # Obtain classifiers
     id.tr          <- id.te <- NULL
     # Loop once for each unique label value
  
     y <- sample(sample(sample(y)))
  
     for(j in 1:length(labels)) 
     {
      sj    <- which(y==labels[j])  # Grab all rows of label type j  
      nj    <- length(sj)           # Count of label j rows to calc proportion below
    
      id.tr <- c(id.tr, (sample(sample(sample(sj))))[1:round(nj*ptr)])
  }                               # Concatenates each label type together 1 by 1
  
  id.te  <- (1:n) [-id.tr]          # Obtain and Shuffle test indices to randomize                                
  
  return(list(idx1=id.tr,idx2=id.te)) 
}  
```

```{r}
  set.seed(19671210)

   library(class)
   library(MASS)

   xtrain <- mnist$train$images
   ytrain <- mnist$train$labels
   ytrain <- as.factor(ytrain)
   
   
   hold  <- stratified.holdout(ytrain, 0.05) 
   id.tr <- hold$idx1
   ntr   <- length(id.tr)
   
   p   <- ncol(xtrain)

   xtest <- mnist$test$images
   ytest <- mnist$test$labels
   ytest <- as.factor(ytest)

   hold  <- stratified.holdout(ytest, 0.05)
   id.te <- hold$idx1
   nte   <- length(id.te)
   
  
   xtr <- xtrain[id.tr,]
   ytr <- ytrain[id.tr]
   xte <- xtest[id.te,]
   yte <- ytest[id.te]
   cat("train set size: ", nrow(xtr))
   cat("\n test set size: ", nrow(xte))
   
   new_dataset <-  rbind(xtr, xte)
   #head(new_dataset)
   new_y <-  c(ytr, yte)

   par(mfrow=c(2,2))
   barplot(prop.table(table(new_dataset)))
   #barplot(prop.table(table(ytr)))
   #barplot(prop.table(table(ytest)))
   #barplot(prop.table(table(yte)))
      
   
   
```


MNIST dataset count 60000 images from train dataset and 10000 from the test dataset. That it's too heavy and will take a long time to run, for that reason, we have decided to choose 5% of each dataset and stratified them . Then, we have obtained n = 3000 and m = 500.


## 3. Let S = 50 be the number of random splits of the data into 70% training and 30% Test.

## 1. Build over all the 5 models and compute the test errors for each split, storing the results into a matrix of test errors

```{r}
set.seed(19671210) # Set seed for reproducibility
epsilon <- 0.3     # Proportion of observations in the test set
S <- 50            # Number of replications

# Initialize an empty matrix to store the test errors
test.err <- matrix(0, nrow=S, ncol=5)

# Convert y to a factor for classification
#y.roc <- as.factor(y)
new_data <- rbind(xtr, xte)
new_y <- as.numeric(c(ytr, yte))-1

for (s in 1:S) {
  # Split the data into training and testing sets using stratified holdout
  hold <- stratified.holdout(as.factor(new_y), 1 - epsilon)
  id.tr <- hold$idx1
  id.te <- hold$idx2
  ntr <- length(id.tr)
  nte <- length(id.te)
  
  y.te <- new_y[id.te]  # True responses in test set
  xtr <- new_data[id.tr, ]
  xte <- new_data[id.te, ]
  ytr <- new_y[id.tr]
  yte <- new_y[id.te]
  
  # First machine: 1NN
    
   yte.knn <- knn(xtr, xte, ytr, k=1)
   conf.mat.te.knn <- table(yte, yte.knn)
   acc.te.knn      <- sum(diag(conf.mat.te.knn))/nte
   test.err[s, 1] <- mean(1 - acc.te.knn)
   
   # First machine: 5NN
    
   yte.knn <- knn(xtr, xte, ytr, k=5)
   conf.mat.te.knn <- table(yte, yte.knn)
   acc.te.knn      <- sum(diag(conf.mat.te.knn))/nte
   test.err[s, 2] <- mean(1 - acc.te.knn)
   
   # First machine: 7NN
    
   yte.knn <- knn(xtr, xte, ytr, k=7)
   conf.mat.te.knn <- table(yte, yte.knn)
   acc.te.knn      <- sum(diag(conf.mat.te.knn))/nte
   test.err[s, 3] <- mean(1 - acc.te.knn)
   
   # First machine: 9NN
    
   yte.knn <- knn(xtr, xte, ytr, k=9)
   conf.mat.te.knn <- table(yte, yte.knn)
   acc.te.knn      <- sum(diag(conf.mat.te.knn))/nte
   test.err[s, 4] <- mean(1 - acc.te.knn)
   
   # First machine: 13NN
    
   yte.knn <- knn(xtr, xte, ytr, k=13)
   conf.mat.te.knn <- table(yte, yte.knn)
   acc.te.knn      <- sum(diag(conf.mat.te.knn))/nte
   test.err[s, 5] <- mean(1 - acc.te.knn)
    
   
  }
  
  test <- data.frame(test.err)
  Method<-c('1NN', '5NN', '7NN', '9NN', '13NN')
  colnames(test) <- Method
  #boxplot(test,
          #ylim = (0, 0.5))
  test
```

## 2. Identify the machine with the smallest median test error and generate the test confusion matrix from the last split

```{r}
boxplot(test, 
        main = "Test Errors for kNN Models", 
        ylab = "Test Error (L2 Loss)", 
        col = rainbow(3), 
        ylim = c(0, 0.2)) 
```

By using the boxplot we can observe that 1NN is the model with the lowest median, so, we are going to use 1NN on the next question 


```{r}
yte.knn <- knn(xtr, xte, ytr, k=1)
conf.mat.te.knn <- table(yte, yte.knn)
conf.mat.te.knn
```

## 3. Comment on the digits for which there is a lot more confusion. Does that agree with your own prior intuition about digits?




The digits for which there is a lot more confusion are 9 and 4  are the digits with the more misclassification:

We have 11 digits 4 predicted as 9 ; also we have 6 missclassification of 8 as 5.

At priory I expected that 6 and 9 would have been confused. But the confusion of 4 and 9 is understandable because depending on the writing hand they can appear similar


## 4. Perform an ANOVA of the test errors and comment on the patterns that emerge.

```{r}
library(reshape2)
 aov.method <- aov(value~variable, data=melt(test))
  anova(aov.method)
  #summary(aov.method)
  
  TukeyHSD(aov.method, ordered = TRUE)
  #plot(TukeyHSD(aov.method))
```


# Part 2: Binary classification on MNIST

Consider classifying digit ’1’ against digit ’7’, with ’1’ representing positive and ’7’ representing
negative. You will be using just 1NN, 5NN, 7NN, 9NN, and 13NN.

## 1. Store in memory your training set and your test set. Of course you must show the
## command that extracts only ’1’ and ’7’ from both the training and the test sets.


```{r}
# Load necessary libraries
#library(keras)
library(dplyr)
#y.roc <- as.factor(y)
new_data <- rbind(xtr, xte)
new_y <- as.numeric(c(ytr, yte))-1

epsilon <- 0.3

 hold <- stratified.holdout(as.factor(new_y), 1 - epsilon)
  id.tr <- hold$idx1
  id.te <- hold$idx2
  ntr <- length(id.tr)
  nte <- length(id.te)
  
  y.te <- new_y[id.te]  # True responses in test set
  xtr <- new_data[id.tr, ]
  xte <- new_data[id.te, ]
  ytr <- new_y[id.tr]
  yte <- new_y[id.te]

# Use 'which' to filter for digits '1' and '7'
train_indices <- which(ytr %in% c(1, 7))
test_indices <- which(yte %in% c(1, 7))

# Create training and test sets
train_set <- as.data.frame(xtr[train_indices,])
train_labels <- factor(ytr[train_indices], levels = c(1, 7))
test_set <- as.data.frame(xtr[test_indices,])
test_labels <- factor(ytr[test_indices], levels = c(1, 7))

# Store the training and test sets
#train_set
head(train_labels)
#test_set
#test_labels
```


## 2. Display both your training confusion matrix and your test confusion matrix

```{r}
# Load additional libraries
library(class)


# Run kNN for different values of k
k_values <- c(1, 5, 7, 9, 13)

for (i in k_values) {
  cat("The training confusion matrix for i = ", i , "\n")
   yte.knn <- knn(train_set, train_set, train_labels, k=i)
   conf.mat.te.knn <- table(train_labels, yte.knn)
   print(conf.mat.te.knn)
   
   cat("The test confusion matrix for i = ", i , "\n")
   yte.knn <- knn(train_set, test_set, train_labels, k=i)
   conf.mat.te.knn <- table(test_labels, yte.knn)
   print(conf.mat.te.knn)
}

```





## 3. Display the comparative ROC curves of the five learning machines



## 4. Identify two false positives and two false negatives at the test phase, and in each case,
## plot the true image against its falsely predicted counterpart.

```{r}
# Load necessary library
library(class)

# Example: Predict using k-NN with a chosen k value 
k <- 1
predicted_labels <- knn(train_set, test_set, train_labels, k = k)

# Identify false positives and false negatives
false_positives <- which(predicted_labels == "1" & test_labels == "7")
false_negatives <- which(predicted_labels == "7" & test_labels == "1")

# Print counts of false positives and false negatives
cat("Number of false positives:", length(false_positives), "\n")
cat("Number of false negatives:", length(false_negatives), "\n")

# Check if we have enough samples
if (length(false_positives) < 2) {
  cat("Not enough false positives to plot. Showing available false positives:\n")
  print(false_positives)
}
if (length(false_negatives) < 2) {
  cat("Not enough false negatives to plot. Showing available false negatives:\n")
  print(false_negatives)
}

# Select two false positives and two false negatives
false_positive_samples <- false_positives[1:min(2, length(false_positives))]
false_negative_samples <- false_negatives[1:min(2, length(false_negatives))]

# Set up plotting area
par(mfrow = c(2, 2))  # 2 rows, 2 columns for the plots

# Plotting false positives
for (i in 1:length(false_positive_samples)) {
  idx <- false_positive_samples[i]
  true_image <- matrix(as.numeric(test_set[idx, ]), nrow = 28)  # Adjust dimensions if necessary
  
  # Check if the image has valid data
  if (any(is.na(true_image))) {
    next  # Skip this iteration if the image is invalid
  }
  
  # Plot true image (label 7) and predicted image (label 1)
  image(true_image, axes = FALSE, col = grey.colors(256))
  title(main = paste("True: 7\nPredicted: 1"))
}

# Plotting false negatives
for (i in 1:length(false_negative_samples)) {
  idx <- false_negative_samples[i]
  true_image <- matrix(as.numeric(test_set[idx, ]), nrow = 28)  # Adjust dimensions if necessary
  
  # Check if the image has valid data
  if (any(is.na(true_image))) {
    next  # Skip this iteration if the image is invalid
  }
  
  # Plot true image (label 1) and predicted image (label 7)
  image(true_image, axes = FALSE, col = grey.colors(256))
  title(main = paste("True: 1\nPredicted: 7"))
}
```



## 5. Comment on any pattern that might have emerged.

In our dataset there is no 1 that was predicted as 7. 
So we were be able to represent only the pattern 7 predicted as 1.

The pattern that have emerged are the pattern 7 such that they can be assimilated to 1 cause of the hand written

## Exercise 3: Video component

My video is focused on the first exercise and is accessible by this link:

[]



