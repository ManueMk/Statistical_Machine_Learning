---
title: "Assignment 1: Statistical Machine Learning"
author: "Manuella Kristeva NAKAM"
date: "2024-12-09"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{R, echo=FALSE}
# Rename the alpha function from kernlab
boot_logit <- boot::logit
boot_melanoma <- boot::melanoma
ggplot2_alpha <- ggplot2::alpha
```

```{R}
suppressMessages(suppressPackageStartupMessages(suppressWarnings({
  library(MASS)
  library(datasets)
  library(ggplot2)
  library(kernlab)
  library(mlbench)
  library(car)
  library(boot)
  library(naivebayes)
  library(dplyr)
  library(psych)
  library(pROC)
  library(caret)
  library(class)
  library(reshape2)
  library(ROCR)
})))


```

\section*{1. Loading the Data}

Download the file \texttt{aims-sml-2024-2025-data.csv} and load it into RMarkdown.

```{r}
aims_sml_data <- read.csv("aims-sml-2024-2025-data.csv")
head(aims_sml_data)
tail(aims_sml_data)
```



```{r}
x = aims_sml_data$x
y = aims_sml_data$y
```


\section*{2. Determine the Size of the Dataset \( n \).}

```{r}
n = dim(aims_sml_data)[1]
p = dim(aims_sml_data)[2] - 1
cat("The size  n  of the dataset is: ", n)
```

\section*{3. Create a Scatterplot of \( y \) Versus \( x \).}

```{r}
plot(y ~ x, data = aims_sml_data,
     main = "Scatterplot of  y  Versus  x ",
     xlab = "X",
     ylab = "Y",
     type = "p")
```

\section*{4. Determine Whether This is a Classification or Regression Task.}

The relationship between \( x \) and \( y \) is given by \( f \):

\[
f(x):
\begin{cases} 
    \mathbf{X} \to y \\
    x \to f(x) \in \mathbf{R}
\end{cases} 
\]

\section*{Part 2: Theoretical Framework}

\subsection*{1. Suggest a Function Space \( H \) for This Task}

From the scatterplot, we can observe that \( x \) and \( y \) are not linearly correlated, and we know that all functions can be approximated by a polynomial.

The function space \( H \) for this task can be defined as:

\[
H := \{f \mid \forall x \in \mathbf{X}: f(x) = \sum_{j=0}^p a_j x^j \}
\]

\subsection*{2. Specify the Loss Function for This Task}

```{r}
boxplot(y)
```

For this regression task, we will use the L2-Loss:

\[
\mathcal{L}(y, f(x)) = (y - f(x))^2
\]

By observing the boxplot of the response variable, we can note the presence of outliers. Despite L2-Loss being sensitive to outliers (since it squares the errors), it has the advantage of being differentiable everywhere, allowing us to minimize it using optimization algorithms.

\subsection*{3. Theoretical Risk \( R(f) \)}

\[
R(f) = \mathbf{E}[\mathcal{L}(y, f(x))] = \mathbf{E}[(y - f(x))^2] = \int \int_{X \times Y} \mathcal{L}(y, f(x)) p_{XY}(x, y) \, dx \, dy
\]

\subsection*{4. Expression for the Bayes Learning Machine \( f^*(x) \)}

\[
f^*(x) = \mathbf{E}[y \mid x] = \int_{Y \mid X} y p_Y(y \mid x) \, dy = \int_{Y \mid X} y \frac{p_{X,Y}(x, y)}{p_X(x)} \, dy
\]

\subsection*{5. Empirical Risk \( \widehat{R}(f) \)}

\[
\widehat{R}(f) = \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y_i, \widehat{f}(x_i)) = \frac{1}{n} \sum_{i=1}^n (y_i - f(x_i))^2
\]

\[
= \frac{1}{n} \left( \mathbf{y} - \mathbf{a}^T \mathbf{x} \right)^2 = \frac{1}{n} \left( \mathbf{y} - \mathbf{a}^T \mathbf{x} \right)^T \left( \mathbf{y} - \mathbf{a}^T \mathbf{x} \right)
\]

\[
= \frac{1}{n} \left( \mathbf{y} - \mathbf{x} \mathbf{a} \right)^T \left( \mathbf{y} - \mathbf{x} \mathbf{a} \right)
\]

Where 

\[
\mathbf{x} = \begin{bmatrix} 
    1 & x_1 & x_1^2 & \ldots & x_1^p \\
    1 & x_2 & x_2^2 & \ldots & x_2^p \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_n & x_n^2 & \ldots & x_n^p 
\end{bmatrix}
\]

and 

\[
\mathbf{a} = (a_0, a_1, \ldots, a_p)^T
\]

\section*{Part 3: Estimation and Model Complexity}

\subsection*{1. Expression for the OLS Estimator \( \widehat{f}(x) \)}

\[
\widehat{f}(x) = \arg \min_{f \in H} (\widehat{R}(f)) 
\]

\[
\frac{\partial \widehat{R}(a)}{\partial a} = 0 \implies \frac{\partial \frac{1}{n}(\mathbf{y} - \mathbf{x} \mathbf{a})^T (\mathbf{y} - \mathbf{x} \mathbf{a})}{\partial a} = 0
\]

\[
\implies \frac{2}{n} \mathbf{x}^T (\mathbf{y} - \mathbf{x} \mathbf{a}) = 0 
\]

\[
\implies \mathbf{x}^T \mathbf{y} - \mathbf{x}^T \mathbf{x} \mathbf{a} = 0 
\]

If \( \mathbf{x}^T \mathbf{x} \) is invertible:

\[
\implies \widehat{\mathbf{a}} = (\mathbf{x}^T \mathbf{x})^{-1} \mathbf{x}^T \mathbf{y} 
\]

\[
\implies \widehat{f}(x) = \sum_{j=0}^p \widehat{a}_j x^j = \mathbf{x} \widehat{\mathbf{a}} 
\]

Where \( \widehat{\mathbf{a}} = (\widehat{a}_0, \widehat{a}_1, \ldots, \widehat{a}_p)^T \).

\subsection*{2. Properties of \( \widehat{f}(x) \)}

(a) The bias of the estimator:

\[
\text{Bias}[\widehat{f}(x)] = \mathbf{E}[\widehat{f}(x)] - f(x) 
\]

Thus, \( \widehat{f}(x) \) is an unbiased estimator because the expectation of \( \widehat{f}(x) \) converges to the true function \( f \).

(b) The minimum variance unbiased estimator:

\[
\text{Var}[\widehat{f}(x)] = \mathbf{E}\left[(\widehat{f}(x) - \mathbf{E}[\widehat{f}(x)])^2\right]
\]

There is uncertainty in the predictions depending on the distribution of the predictors. An estimator with less variance will have individual data points closer to the mean, leading to more accurate results.

(c) The consistency of the estimator:

As the sample size increases, the estimator \( \widehat{f}(x) \) converges to the true function \( f \). Thus, with a large dataset, the model will be more accurate.

Each property of the OLS estimator adds restrictions to the model while allowing for stronger statements regarding OLS.


3. Use V -fold cross-validation (e.g., V =, 5, 10) to determine the optimal complexity (degree
p) for the polynomial regression model. Explain what “optimal complexity” means.


```{r}
set.seed(19671210)

cv_error <- function(deg, data, V){
  n <- nrow(data)
  chunks <- sample(rep(1:V, length.out = n))
  errors <- numeric(V)
  for (v in 1:V) {
    train <- data[chunks != v, ]
    test <- data[chunks == v, ]
    
    model <- lm(y ~ poly(x, deg), data = train)
    predictions <- predict(model, newdata = test)
    errors[v] <- mean((predictions -test$y)^2)
    
  }
  return( mean(errors))
}

my_data <- data.frame(x, y)

degrees <- 1:25
V_values <- c(5, 10,15)

cv_errors_list <- list()
optimal_degree <- list()

for (V in V_values) {
  cv_errors <- sapply(degrees, function(degree) cv_error(degree, my_data, V))
  cv_errors_list[[as.character(V)]] <- cv_errors
  # Find optimal degree
  optimal_degree[[as.character(V)]] <- degrees[which.min(cv_errors)]
}

# Plot cross-validation errors for each V
plot(degrees, cv_errors_list[[1]], type = "b", col = "blue", xlab = "Degree of Polynomial", 
     ylab = "Cross-Validation Error", main = "Cross-Validation Error vs Degree", ylim = c(0, max(unlist(cv_errors_list))))
lines(degrees, cv_errors_list[[2]], type = "b", col = "red")
lines(degrees, cv_errors_list[[3]], type = "b", col = "green")
abline(v = 10, col = "orange")
text(optimal_degree, max(cv_errors), labels = paste("Optimal Degree: 10"), pos = 4, col = "orange")

# Add legend
legend("topright", legend = paste("CV with V =", V_values), col = c("blue", "red", "green"), lty = 1)

for (V in V_values) {
  cat(sprintf("Cross-Validation Errors for V = %d:\n", V))
  
  cat(sprintf("The optimal degree is : %d \n", optimal_degree[[as.character(V)]]))
  
}

```


“optimal complexity” means the number of parameters of the function. 

Generally when the complexity is small we have small variance and large bias
When is large we have large variance and large bias
The sweet complexity is between the two. To find it we have Minimum Variance Unbiased Estimator

In our case: (The degree can change at each training but we have choose the one that have appeared many times )
- The optimal complexity for V = 5 is : 21 
- The optimal complexity is for V = 10 : 10 
- The optimal complexity  for V = 15 is: 17 

Also, we can see on the plot that the cross validation error is almost the same for V = 5 and V = 15 but after the degree 20 the errors is increasing so the model start to overfit.

With V = 10 even when allow us to observe that after 20 the model becomes too complex the error is reasonable.

For that reason we are going to choose the model obtain when V= 10 because it's not highly affected by the complexity of the model.



4. Plot the cross-validation error and empirical risk as functions of p. Comment on the plot.

```{r}
set.seed(19671210)

my_data <- data.frame(x, y)

degrees <- 1:25
cv_errors <- sapply(degrees, function(d) cv_error(d, my_data, V = 10))

# Calculate empirical risk 
empirical_risks <- sapply(degrees, function(degree) {
  model <- lm(y ~ poly(x, degree), data = my_data)
  mean((predict(model) - my_data$y)^2)
})

# Find optimal degree
optimal_degree <- degrees[which.min(cv_errors)]

# Plot both errors
plot(degrees, cv_errors, type = "b", col = "blue", ylim = range(c(cv_errors, empirical_risks)), 
     ylab = "Error", xlab = "Degree of Polynomial", 
     main = "Cross-Validation Error vs Degree")
lines(degrees, empirical_risks, type = "b", col = "red")
legend("topright", legend = c("Cross-Validation Error", "Empirical Risk"), col = c("blue", "red"), lty = 1)
abline(v = 10, col = "orange")
text(optimal_degree, max(cv_errors), labels = paste("Optimal Degree: ", 10), pos = 2, col = "orange")



```


As the degree increases, both cross validation and empirical risk decrease initially indicating that more the model is complex  it fit better. After certain points the cross validation start increasing that indicate overfitting.


Part 4: Model Comparison and Evaluation
1. Fit and plot the following models on the same plot with the data:
• The simplest estimator $\hat{f(x)}$ that depends on x.
• The optimal estimator determined by cross-validation.
• An overly complex model.
Use a legend to distinguish the models and comment on their behaviors.

```{r}
# Load necessary libraries


# Generate example data
set.seed(19671210)


# Prepare data frame
data <- data.frame(x = x, y = y)

# Fit the simplest estimator 
simple_model <- lm(y ~ poly(x, 3), data = data)

# Fit the optimal model 
optimal_model <- lm(y ~ poly(x, 10), data = data)

# Fit the overly complex model 
complex_model <- lm(y ~ poly(x, 23), data = data)

# Create predictions for plotting
data$simple_pred <- predict(simple_model, newdata = data)
data$optimal_pred <- predict(optimal_model, newdata = data)
data$complex_pred <- predict(complex_model, newdata = data)

# Plotting
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "grey") +
  geom_line(aes(y = simple_pred, color = "Simple Model (Degree 3)"), size = 1) +
  geom_line(aes(y = optimal_pred, color = "Optimal Model (Degree 10)"), size = 1) +
  geom_line(aes(y = complex_pred, color = "Complex Model (Degree 23)"), size = 1) +
  labs(title = "Model Comparison",
       x = "x",
       y = "y") +
  scale_color_manual(values = c("blue", "red", "green")) +
  theme_minimal() +
  theme(legend.title = element_blank())
```

-The simple model is not able to fit well the train data that implies underfitting

- The optimal model is able to fit well the data and can be useful on unknown data 

-The complex model fit well the data but also fit the noise that implies overfitting and make it unable to be use on unknown data.

Also, the optimal and the complex are almost the same at the median


2. Perform stochastic hold-out validation with S = 100 splits (70% training, 30% testing).
Compute and plot boxplots of the test errors for:
• The simplest model.
• The optimal model.
• The overly complex model.

```{R}
  set.seed (19671210)          # Set seed for random number generation to be reproducible
  
  epsilon <- 3/10               # Proportion of observations in the test set
  nte     <- round(n*epsilon)  # Number of observations in the test set
  ntr     <- n - nte
  
  S <- 100   # Number of replications
  test.err <- matrix(0, nrow=S, ncol=3)
  
  for(s in 1:S)
  {
    # Split the data
    
    id.tr   <- sample(sample(sample(n)))[1:ntr]                   # For a sample of ntr indices from {1,2,..,n}
    id.te   <- setdiff(1:n, id.tr)
    
    y.te         <- y[id.te]                                        # True responses in test set
    
    # First machine: simple_model
    
                      # Random variable tracking error. Indicator
    simple_model <- lm(y ~ poly(x, 3), data = data.frame(x = x[id.tr], y = y[id.tr]))
    y.te.hat <- predict(simple_model, newdata = data.frame(x = x[id.te]))
    test.err[s, 1] <- mean((y.te - y.te.hat)^2)  # Mean Squared Error
    
    
    
    # Second machine: optimal_model
    
    optimal_model <- lm(y ~ poly(x, 10), data = data.frame(x = x[id.tr], y = y[id.tr]))
    y.te.hat <- predict(optimal_model, newdata = data.frame(x = x[id.te]))
    test.err[s, 2] <- mean((y.te - y.te.hat)^2)  # Mean Squared Error
    
      
    # Third machine: complex_model
    
    complex_model <- lm(y ~ poly(x, 25), data = data.frame(x = x[id.tr], y = y[id.tr]))
    y.te.hat <- predict(complex_model, newdata = data.frame(x = x[id.te]))
    test.err[s, 3] <- mean((y.te - y.te.hat)^2)  # Mean Squared Error
  }
  
    
  test <- data.frame(test.err)
  Method<-c('simple', 'optimal', 'complex')
  colnames(test) <- Method
  #boxplot(test,
          #ylim = (0, 0.5))
  boxplot(test, 
        main = "Test Errors for Polynomial Models", 
        ylab = "Test Error (L2 Loss)", 
        col = rainbow(3), 
        ylim = c(0, 0.2)) 
```  

We observe that:
- The simple model is the worst on because the error large the other

Statistically the optimal and the complex are almost the same but the differences are:

-The optimal model have a minimum variance error because all the quartiles are close and all the errors are closed to 0

- The complex model have a lot of outliers and the variance of error is too large that implies overfitting

The optimal model seems to be the best model 

Part 5: Further Analysis
1. Perform an analysis of variance (ANOVA) on the test errors. What does it reveal?


```{r}
head(test)
aov.method <- aov(value~variable, data=melt(test))
anova(aov.method)
```
The p-value of 0.04583 less than 0.05, so we reject the null hypothesis, which implies  that there are statistically significant differences in the mean values of the errors depending on each model . 




2. Obtain and plot the 95% confidence and prediction bands for the dataset Dn.





```{r}
  
TukeyHSD(aov.method, ordered = TRUE)
  plot(TukeyHSD(aov.method))
```
There is a statistically significant difference between the 'complex' and 'optimal' groups.
There are no significant differences between 'simple' and 'optimal', or between 'complex' and 'simple'.

In contrast, the 'simple' group does not significantly differ from either the 'optimal' or 'complex' groups, suggesting that the 'simple' approach may yield similar outcomes to 'optimal' but lacks a distinct impact compared to 'complex'.


```{R, echo=FALSE}



# Define the response and best predictor
response <- aims_sml_data$y
best_predictor_values <- aims_sml_data$x

# Fit the simple linear regression model for the best predictor
best_model <- lm(y ~ poly(x, 10), data = aims_sml_data)

# Generate a sequence of predictor values for smoother bands
x_new <- seq(min(best_predictor_values), max(best_predictor_values), length.out = 100)

# Create a new data frame for predictions
#new_data <- data.frame(best_predictor_values = x_new)
new_data <- data.frame(x = x_new)

# Compute the confidence intervals and prediction intervals
predictions <- predict(
  best_model, 
  newdata = new_data, 
  interval = "confidence", # Confidence bands
  level = 0.95             # 95% confidence level
)

predictions_pred <- predict(
  best_model, 
  newdata = new_data, 
  interval = "prediction", # Prediction bands
  level = 0.95             # 95% prediction level
)

# Plot the data and the regression line
plot(
  x, y,
  main = paste("Confidence and Prediction Bands for x"),
  xlab = "x",
  ylab = "y",
  pch = 16, col = "blue"
)

# Add the regression line
lines(x_new, predictions[, "fit"], col = "red", lwd = 2)

# Add the confidence bands
lines(x_new, predictions[, "lwr"], col = "darkgreen", lwd = 2, lty = 2)
lines(x_new, predictions[, "upr"], col = "darkgreen", lwd = 2, lty = 2)

# Add the prediction bands
lines(x_new, predictions_pred[, "lwr"], col = "orange", lwd = 2, lty = 3)
lines(x_new, predictions_pred[, "upr"], col = "orange", lwd = 2, lty = 3)

# Add a legend
legend(
  "topleft",inset=0.02,
  legend = c("Regression Line", "Confidence Bands", "Prediction Bands"),
  col = c("red", "darkgreen", "orange"),
  lty = c(1, 2, 3),
  lwd = 2,
  bty = "n"
)

```


3. Write the mathematical expression for:
• The confidence band for a single observation (Xi, Yi).

\[
\widehat{Y}_i \pm t_{\alpha/2, n-p} \cdot SE(\widehat{Y}_i)\\
Where\quad  SE(\hat{Y}_i) = standard \quad error \quad and \quad t_{\alpha/2, n-p} = Critical \quad value-t
\]

• The prediction band for a single observation (Xi, Yi).

\[
\widehat{Y}_i \pm t_{\alpha/2, n-p} \cdot \sqrt{SE^2(\hat{Y}_i) + \sigma^2}\\
Where\quad  SE(\widehat{Y}_i) = standard \quad error \quad \\and \quad t_{\alpha/2, n-p} = Critical \quad value-t \quad and \quad \sigma^2 \quad variance \quad errors
\]

4. Comment extensively on what the confidence and prediction bands reveal about the
model.

- Confidence bands show us how well we know the location of the best fit line or curve. Given all the assumptions of the analysis, we can be 95% sure that the true curve (nonlinear regression) or line (linear regression) lies within the bands: the uncertainty in the position of the line or curve .

- Prediction bands show us where we can expect the data to lie. You expect 95% of all data points to lie within the prediction bands. 

# Exercise 2: 

Consider the spam dataset from library(kernlab). You are supposed to provide a thorough comparison of four learning machines namely LDA, QDA, Naive Bayes and FLD, and your comparison
will be solely based on the test error.



```{r}
data(spam)        # load the data
xy <- spam        # Store data in xy frame
#help(spam)        # learn stuff about this dataset
```

```{r}
dim(spam)
#View(spam)
```





```{R}
  n   <- nrow(xy)       # Sample size
  p   <- ncol(xy) - 1   # Dimensionality of the input space
  pos <- p+1            # Position of the response
  x   <- xy[,-pos]      # Data matrix: n x p matrix
  y   <- xy[, pos]      # Response vector
  colnames(xy)[pos] <- 'y'
  xy[, pos] <- ifelse(xy[,pos]==unique(y)[1], 0, 1)
  y   <- as.factor(xy[, pos])      # Response vector
  #n; p; xy[,pos]
```


1. Plot the distribution of the response for this dataset and comment.

```{r}
table(y)
barplot(table(y), xlab = 'Spam')
```

The class is imbalance because we have more of spam than no-spam in the dataset. 


2. Comment on the shape of this dataset in terms of the sample size and the dimensionality
of the input space

```{r}
cat("The sample Size (n):", n, "\n")
cat("The dimensionality (p):", p, "\n")
```

The sample size of our dataset is 4601 that is good because it allows to increases the accuracy. The model is going to be able to be train on a data varied and the larger samples tend to reduce the impact of random variation, leading to more stable and reliable estimates of population parameters.

The dimensionnality is 57 , we are not going to be able to visualize well the data and identify the noise variables. 

3. Comment succinctly from the statistical perspective on the type of data in the input space

```{R}
str(xy)
```


All the 57 variables are numerical and the response variable "spam" is categorical binary . That implies that we are going to do binary classification



```{r}
colSums(is.na(xy))
```
THere is no missing values in the whole dataset.

```{r}
 par(mfrow=c(2,3))
 for(j in 1:9)
 {
   boxplot(x[,j]~y, col=2:3, ylab=colnames(x)[j], xlab='spam')
 }   
```

4. Using the whole data for training and the whole data for test, building the above four
learning machines, then plot the comparative ROC curves on the same grid



```{r}
roc_data <- list()

set.seed(19671210)

#lda
lda.m1 <- lda(y ~ ., data = x)
test.predicted.lda <- predict(lda.m1, newdata = x)
lda_roc <- roc(y,  test.predicted.lda$posterior[,2])

#qda
qda.m1 <- qda(y ~ ., data = x)
test.predicted.qda <- predict(qda.m1, newdata = x)
qda_roc <- roc(y,  test.predicted.qda$posterior[,2])

#naive_bayes
nb.m1 <- naive_bayes(y ~ ., data = x, usekernel = T)
test.predicted.nb <- predict(nb.m1, newdata = x, type = "prob")[,2]
nb_roc <- roc(y,  test.predicted.nb)

#fld
fld.m1 <- lda(y ~ ., data = x)
test.predicted.fld <- predict(fld.m1, newdata = x)
fld_roc <- roc(y,  test.predicted.fld$posterior[,2])


# Plot ROC curves
plot(lda_roc, col = "blue", main = "ROC Curves", lwd = 2)
lines(qda_roc, col = "red", lwd = 2)
lines(nb_roc, col = "green", lwd = 2)
lines(fld_roc, col = "purple", lwd = 2)
legend("bottomright", legend = c("LDA", "QDA", "Naive Bayes", "FLD"),
       col = c("blue", "red", "green", "purple"), lwd = 2)

# Calculate and print AUC values for each model
auc_values <- c(LDA = auc(lda_roc), QDA = auc(qda_roc), NaiveBayes = auc(nb_roc))
print(auc_values)

```

5. Comment succinctly on what the ROC curves reveal for this data and argue in light of
the theory whether or not that was to be expected.

Naive Bayes is the model with the largest area under the curve, that implies that it's the best one.

Concerning the other they are almost the same.

6. Using set.seed(19671210) along with a 2/3 training 1/3 test in the context stratified
stochastic holdout split of the data, compute S = 50 replications of the test error for all
the above learning machines.

```{r}
stratified.holdout <- function(y, ptr)
   {
    n              <- length(y)
    labels         <- unique(y)       # Obtain classifiers
    id.tr <- id.te <- NULL
    
    # Loop once for each unique label value
  
    y <- sample(sample(sample(y)))
  
    for(j in 1:length(labels)) 
    {
      sj    <- which(y==labels[j])  # Grab all rows of label type j  
      nj    <- length(sj)           # Count of label j rows to calc proportion below
    
      id.tr <- c(id.tr, (sample(sample(sample(sj))))[1:round(nj*ptr)])
  }                               # Concatenates each label type together 1 by 1
  
  id.te  <- (1:n) [-id.tr]          # Obtain and Shuffle test indices to randomize                                
  
  return(list(idx1=id.tr,idx2=id.te)) 
}  
```


```{R}


set.seed(19671210) # Set seed for reproducibility
epsilon <- 1/3     # Proportion of observations in the test set
R <- 50            # Number of replications

# Initialize an empty matrix to store the test errors
test.err <- matrix(0, nrow=R, ncol=3)

# Convert y to a factor for classification
y.roc <- as.factor(y)

for (r in 1:R) {
  # Split the data into training and testing sets using stratified holdout
  hold <- stratified.holdout(as.factor(xy[, pos]), 1 - epsilon)
  id.tr <- hold$idx1
  id.te <- hold$idx2
  ntr <- length(id.tr)
  nte <- length(id.te)
  
  y.te <- y[id.te]  # True responses in test set
  
  # 1. Linear Discriminant Analysis (LDA)
  lda.mod <- lda(x[id.tr,], y.roc[id.tr])
  y.te.hat <- predict(lda.mod, x[id.te,])$class
  ind.err.te <- ifelse(y.te != y.te.hat, 1, 0)  # Indicator variable tracking errors
  test.err[r, 1] <- mean(ind.err.te)
  
  # 2. Quadratic Discriminant Analysis (QDA)
  qda.mod <- tryCatch({
    qda(x[id.tr, ], y.roc[id.tr], method = "moment")
  }, error = function(e) {
    NULL
  })
  if (!is.null(qda.mod)) {
    y.te.hat <- predict(qda.mod, x[id.te, ])$class
    ind.err.te <- ifelse(y.te != y.te.hat, 1, 0)
    test.err[r, 2] <- mean(ind.err.te)
  } else {
    test.err[r, 2] <- NA
  }
  
  
  
  # 3. Naive Bayes (NB)
  naive.mod <- naive_bayes(as.factor(y) ~ ., data = xy[id.tr, ])
  y.te.hat <- predict(naive.mod, x[id.te,], type = 'class')
  ind.err.te <- ifelse(y.te != y.te.hat, 1, 0)  # Indicator variable tracking errors
  test.err[r, 3] <- mean(ind.err.te)
}

# Set column names for the error matrix
Method <- c('LDA', 'QDA', 'Naive_Bayes')
colnames(test.err) <- Method


```

  


7. Plot the comparative boxplots (be sure to properly label the plots)

```{r}
boxplot(test.err)

```

8. Comment on the distribution of the test error in light of (implicit) model complexity.

According to the boxplot we can observe  that LDA is the best model on the threshold and naive bayes the worst.

We remember that when we have train and test on the same dataset naive bayes was the best, That confirm  that being the best model on the train dataset doesn't implies that you are a predictor.

